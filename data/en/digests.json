{
  "pageTitle": "Digests - Excursion Studio BETA",
  "hero": {
    "title": "Excursion Studio - Digests",
    "description": "Stay tuned for Excursion Studio's upcoming <strong>Paper Guide</strong> and <strong>Paper Express</strong> series columns!"
  },
  "ui": {
    "showMore": "Show More",
    "showLess": "Show Less",
    "readMore": "Read More"
  },
  "sections": [
    {
      "type": "digests",
      "id": "all-digests",
      "items": [
        {
          "category": "paper-guide",
          "categoryName": "Paper Guide",
          "number": 1,
          "title": "Attention Is All You Need",
          "description": "This paper proposes a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show that these models are superior in quality while being more parallelizable and requiring significantly less time to train.",
          "date": "2017-06-12",
          "digestPubTime": "2026-02-18",
          "authors": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar"],
          "tags": ["Transformer", "Attention", "NLP", "Deep Learning"],
          "venue": "NeurIPS 2017",
          "pdfUrl": "https://arxiv.org/abs/1706.03762",
          "sourcePath": "paper-guide/papers/attention-is-all-you-need/attention-is-all-you-need.md"
        },
        {
          "category": "paper-express",
          "categoryName": "Paper Express",
          "number": 1,
          "title": "BERT: Pre-training of Deep Bidirectional Transformers",
          "description": "This paper introduces a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers.",
          "date": "2018-10-11",
          "digestPubTime": "2026-02-17",
          "authors": ["Jacob Devlin", "Ming-Wei Chang", "Kenton Lee"],
          "tags": ["BERT", "Pre-training", "NLP", "Transformer"],
          "venue": "NAACL 2019",
          "pdfUrl": "https://arxiv.org/abs/1810.04805",
          "sourcePath": "paper-express/papers/bert/bert.md"
        }
      ]
    }
  ]
}
