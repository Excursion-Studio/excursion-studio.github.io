{
  "pageTitle": "文摘 - 远行工作室 测试版",
  "hero": {
    "title": "远行工作室 - 文摘系列",
    "description": "敬请期待远行工作室即将推出的<strong>《论文导读》</strong>和<strong>《论文速递》</strong>系列栏目！"
  },
  "ui": {
    "showMore": "展开更多",
    "showLess": "收起",
    "readMore": "阅读全文"
  },
  "sections": [
    {
      "type": "digests",
      "id": "all-digests",
      "items": [
        {
          "category": "paper-guide",
          "categoryName": "论文导读",
          "number": 1,
          "title": "Attention Is All You Need",
          "description": "本文提出了一种新的简单网络架构——Transformer，完全基于注意力机制，摒弃了循环和卷积。在两个机器翻译任务上的实验表明，这些模型在质量上优越，同时更易于并行化，训练时间显著减少。",
          "date": "2017-06-12",
          "digestPubTime": "2026-02-18",
          "authors": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar"],
          "tags": ["Transformer", "Attention", "NLP", "Deep Learning"],
          "venue": "NeurIPS 2017",
          "pdfUrl": "https://arxiv.org/abs/1706.03762",
          "sourcePath": "paper-guide/papers/attention-is-all-you-need/attention-is-all-you-need.md"
        },
        {
          "category": "paper-express",
          "categoryName": "论文速递",
          "number": 1,
          "title": "BERT: Pre-training of Deep Bidirectional Transformers",
          "description": "本文介绍了一种新的语言表示模型BERT，它代表来自Transformer的双向编码器表示。与最近的语言表示模型不同，BERT旨在通过在所有层中联合调节左右上下文来预训练深度双向表示。",
          "date": "2018-10-11",
          "digestPubTime": "2026-02-17",
          "authors": ["Jacob Devlin", "Ming-Wei Chang", "Kenton Lee"],
          "tags": ["BERT", "Pre-training", "NLP", "Transformer"],
          "venue": "NAACL 2019",
          "pdfUrl": "https://arxiv.org/abs/1810.04805",
          "sourcePath": "paper-express/papers/bert/bert.md"
        }
      ]
    }
  ]
}
